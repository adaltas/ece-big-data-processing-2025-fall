{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8013b0-a8bd-45d0-a6fb-4e3ef5254482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install libs not already included in instance\n",
    "%pip install requests-sse\n",
    "%pip install pywikibot\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac002f20-02d7-4505-a2ad-2655709f35d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pywikibot.comms.eventstreams import EventStreams\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d764047-b652-4d2c-a3fa-b9eaf0fd12cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the name of the new uc_catalog\n",
    "uc_catalog = 'wikimedia_db'\n",
    "spark.sql('create catalog if not exists ' + uc_catalog)\n",
    "\n",
    "# define the raw events schema\n",
    "uc_schema_raw_events = 'raw_events'\n",
    "spark.sql('create schema if not exists ' + uc_catalog + '.' + uc_schema_raw_events)\n",
    "\n",
    "# save the volume time\n",
    "tmp_volume_time = datetime.now()\n",
    "tmp_volume =  f'events_tmp_{tmp_volume_time.strftime('%y_%m_%d')}'\n",
    "spark.sql('create volume if not exists ' + uc_catalog + '.' + uc_schema_raw_events + '.' + tmp_volume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7bd4239-128e-45c2-bb5c-757fdedbdad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create uc last_event_cache schema and volume if not exists\n",
    "uc_schema_last_event_cache = 'last_event_cache'\n",
    "spark.sql('create schema if not exists ' + uc_catalog + '.' + uc_schema_last_event_cache)\n",
    "\n",
    "last_event_cache_volume = 'data'\n",
    "spark.sql('create volume if not exists ' + uc_catalog + '.' + uc_schema_last_event_cache + '.' + last_event_cache_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97eda7f4-cd1c-4ef0-a360-87ffd368b1db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# simple helper function for checking if a file exists\n",
    "def check_file_exists(last_event_cache_path: str) -> bool:\n",
    "    return os.path.exists(last_event_cache_path)\n",
    "\n",
    "# set stream object to start from  7 days ago on first run\n",
    "# and then from the last event id on subsequent runs\n",
    "def set_stream(last_event_cache_path: str, start_time: datetime) -> EventStreams:\n",
    "    if not check_file_exists(last_event_cache_path):\n",
    "        # start from 7 days ago\n",
    "        stream_start_date_raw = start_time - timedelta(days=8)\n",
    "        stream_start_date_formatted = stream_start_date_raw.strftime('%Y%m%d')\n",
    "        return EventStreams(streams=[\"recentchange\", \"revision-create\"], since=stream_start_date_formatted)\n",
    "    else:\n",
    "        # start from last event id\n",
    "        with open(last_event_cache_path, 'r') as f:\n",
    "            last_time_stamp = f.read()\n",
    "            return EventStreams(streams=[\"recentchange\", \"revision-create\"], since=last_time_stamp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed2dde1e-df9a-43c4-8e2d-527d7a1960b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set start time for streaming and temp voume naming\n",
    "start_time = datetime.now()\n",
    "\n",
    "# set stop time for streaming\n",
    "duration = .5\n",
    "stop_time = start_time + timedelta(minutes=duration)\n",
    "\n",
    "# set last_event_cache path\n",
    "last_event_cache_path = f\"/Volumes/{uc_catalog}/{uc_schema_last_event_cache}/{last_event_cache_volume}/last_event_cache.txt\"\n",
    "\n",
    "# create the streaming object\n",
    "stream = set_stream(last_event_cache_path, start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5ec723-b370-4bd5-b607-85573075f1d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# example filter to only look for edits to fr.wikipedia\n",
    "stream.register_filter(server_name='fr.wikipedia.org', type='edit')\n",
    "\n",
    "# build path for temp volume\n",
    "raw_volume_path = f\"/Volumes/{uc_catalog}/{uc_schema_raw_events}/{tmp_volume}\"\n",
    "\n",
    "# loop to get streaming data\n",
    "while datetime.now() < stop_time:\n",
    "    change = next(stream)\n",
    "    \n",
    "    # Use a field that definitely exists in the event data\n",
    "    event_timestamp = change['meta']['dt']  # ISO 8601 timestamp\n",
    "    revision_id = change.get('revision', {}).get('new', 'unknown')  # More reliable\n",
    "    \n",
    "    file = f\"{raw_volume_path}/event_{revision_id}.json\"\n",
    "\n",
    "    # write event to file\n",
    "    with open(file, 'w') as f:\n",
    "        json.dump(change, f)\n",
    "\n",
    "    # update last_event_cache with TIMESTAMP (what 'since' actually needs)\n",
    "    with open(last_event_cache_path, 'w') as f:\n",
    "        f.write(event_timestamp)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wikimedia_get_raw_stream_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
